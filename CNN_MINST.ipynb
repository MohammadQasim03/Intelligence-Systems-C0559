{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMa2F4fIPeZrJumB1Xs25hT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammadQasim03/Intelligence-Systems-C0559/blob/main/CNN_MINST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWmCgy5M-d52",
        "outputId": "10e26288-4495-421e-fa83-23e3fd4492be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "115/469 [======>.......................] - ETA: 1:54 - loss: 0.9919 - accuracy: 0.6720"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as k\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# Reshape data based on backend (channels-first or channels-last)\n",
        "if k.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    inpx = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    inpx = (img_rows, img_cols, 1)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Define the CNN model\n",
        "inpx = Input(shape=inpx)\n",
        "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\n",
        "layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\n",
        "layer3 = MaxPooling2D(pool_size=(2, 2))(layer2)\n",
        "layer4 = Dropout(0.25)(layer3)\n",
        "layer5 = Flatten()(layer4)\n",
        "layer6 = Dense(128, activation='relu')(layer5)\n",
        "layer7 = Dropout(0.5)(layer6)\n",
        "layer8 = Dense(10, activation='softmax')(layer7)\n",
        "\n",
        "model = Model([inpx], layer8)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=10,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=0.1)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=128),\n",
        "                    epochs=30,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "# Plot accuracy versus time\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Loss:', score[0])\n",
        "print('Accuracy:', score[1])\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g363dtxEXurQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1uCvRvRdXzCw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPBgrWKQX0oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WWCqJnCAX084"
      }
    }
  ]
}